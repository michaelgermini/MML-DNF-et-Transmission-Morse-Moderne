# 8.2 Pipeline : Texte → MML → Compression → Morse

## Le chemin de la transformation : De la source à la transmission

### Vue d'ensemble du pipeline direct

Le pipeline direct représente le **flux de transformation** qui convertit un document source (HTML, Markdown, texte) en signaux Morse transmissibles. Cette chaîne de traitement est conçue pour être :

- **Linéaire** : Chaque étape transforme les données de manière irréversible vers la transmission
- **Optimisée** : Chaque étape réduit la taille et améliore la transmissibilité
- **Robuste** : Gestion d'erreurs et récupération automatique
- **Mesurable** : Métriques à chaque étape pour monitoring et optimisation

## Étape 1 : Acquisition et préparation des données source

### Chargement du document source

#### Formats supportés

**Documents natifs** :
- **HTML** : Pages web, articles, documentation
- **Markdown** : Documents texte structuré
- **Text brut** : Contenu non formaté
- **JSON/XML** : Données structurées

**Métadonnées extraites** :
- **Titre** : Identification du document
- **Auteur** : Origine et responsabilité
- **Date** : Temporalité du contenu
- **Langue** : Pour optimisation lexicale
- **Type MIME** : Nature du contenu

#### Validation initiale

```python
class SourceLoader:
    def __init__(self):
        self.supported_formats = ['html', 'md', 'txt', 'json', 'xml']
        self.max_size = 10 * 1024 * 1024  # 10MB limite

    async def load_document(self, source_path):
        """
        Chargement et validation du document source
        """
        # Détection du format
        format_type = self.detect_format(source_path)
        
        if format_type not in self.supported_formats:
            raise UnsupportedFormatError(f"Format {format_type} non supporté")
        
        # Chargement du contenu
        content = await self.read_content(source_path)
        
        # Validation de taille
        if len(content) > self.max_size:
            raise DocumentTooLargeError(f"Taille {len(content)} > {self.max_size}")
        
        # Extraction des métadonnées
        metadata = self.extract_metadata(content, format_type)
        
        return {
            'content': content,
            'format': format_type,
            'metadata': metadata,
            'size': len(content)
        }
```

### Normalisation préliminaire

#### Nettoyage du contenu

**Opérations** :
- **Encodage** : Conversion en UTF-8
- **Normalisation** : Espaces, sauts de ligne
- **Suppression** : Contenu non essentiel (commentaires, scripts)
- **Validation** : Cohérence structurelle

#### Optimisation pré-MML

**Pré-traitements** :
- **Déduplication** : Contenu répétitif
- **Compression d'images** : Si présentes
- **Extraction de liens** : Résolution des URLs

## Étape 2 : Conversion en MML (Markup Language Layer)

### Sélection du convertisseur

#### Mapping par format

```python
class MMLConverter:
    def __init__(self):
        self.converters = {
            'html': HTMLToMMLConverter(),
            'md': MarkdownToMMLConverter(),
            'txt': TextToMMLConverter(),
            'json': JSONToMMLConverter(),
            'xml': XMLToMMLConverter()
        }

    async def convert_to_mml(self, document_data):
        """
        Conversion vers MML selon le format source
        """
        format_type = document_data['format']
        content = document_data['content']
        
        if format_type not in self.converters:
            raise NoConverterError(f"Pas de convertisseur pour {format_type}")
        
        converter = self.converters[format_type]
        
        # Conversion principale
        mml_content = await converter.convert(content)
        
        # Fusion des métadonnées
        mml_with_meta = self.inject_metadata(mml_content, document_data['metadata'])
        
        return {
            'mml': mml_with_meta,
            'original_size': document_data['size'],
            'mml_size': len(mml_with_meta),
            'compression_ratio': len(mml_with_meta) / document_data['size']
        }
```

### Conversion HTML spécialisée

#### Pipeline HTML→MML

1. **Parsing HTML** : Analyse DOM
2. **Filtrage sémantique** : Suppression du non-essentiel
3. **Mapping structurel** : Conversion balise par balise
4. **Optimisation** : Réduction de verbosité

**Exemple détaillé** :
```python
class HTMLToMMLConverter:
    async def convert(self, html_content):
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Suppression du superflu
        self.remove_non_essential(soup)
        
        # Conversion récursive
        mml_parts = []
        for element in soup.recursiveChildGenerator():
            if isinstance(element, Tag):
                mml_equivalent = self.convert_tag(element)
                mml_parts.append(mml_equivalent)
            elif isinstance(element, NavigableString):
                mml_parts.append(self.convert_text(element))
        
        return ''.join(mml_parts)
```

### Validation MML

#### Contrôles de conformité

**Règles vérifiées** :
- **Syntaxe** : Structure MML valide
- **Sémantique** : Hiérarchie logique
- **Encodage** : Caractères UTF-8 valides
- **Taille** : Limites raisonnables

## Étape 3 : Compression MML-C (Compression Layer)

### Sélection de la stratégie de compression

#### Facteurs de décision

**Contexte de transmission** :
- **Urgence** : MML-C Nano (maximum compression)
- **Fiabilité** : MML-C Robust (redondance)
- **Vitesse** : MML-C Fast (léger)
- **Standard** : MML-C Standard (équilibre)

**Caractéristiques du document** :
- **Taille** : Grosse → compression agressive
- **Type** : Technique → tokens spécialisés
- **Langue** : Vocabulaires adaptés

```python
class CompressionStrategySelector:
    def select_strategy(self, mml_content, transmission_context):
        """
        Sélection automatique de la stratégie de compression
        """
        content_analysis = self.analyze_content(mml_content)
        context_requirements = self.evaluate_context(transmission_context)
        
        if context_requirements['urgency'] == 'high':
            return 'nano'
        elif context_requirements['reliability'] == 'critical':
            return 'robust'
        elif context_requirements['speed'] == 'high':
            return 'fast'
        else:
            return 'standard'
```

### Application de la compression

#### Pipeline de compression modulaire

```python
class MMLCCompressor:
    def __init__(self, strategy='standard'):
        self.strategy = strategy
        self.pipeline = self.build_compression_pipeline()

    def build_compression_pipeline(self):
        """
        Construction du pipeline selon la stratégie
        """
        base_stages = [
            StructuralOptimizer(),
            ShortCodeApplier(),
            LexicalTokenizer()
        ]
        
        if self.strategy == 'nano':
            base_stages.extend([
                VocabularyReducer(),
                ArithmeticEncoder()
            ])
        elif self.strategy == 'robust':
            base_stages.extend([
                RedundancyAdder(),
                ErrorCorrectionCoder()
            ])
        
        base_stages.append(MetadataAttacher())
        return base_stages

    async def compress(self, mml_content):
        """
        Compression complète MML → MML-C
        """
        compressed = mml_content
        
        for stage in self.pipeline:
            compressed = await stage.process(compressed)
        
        return {
            'compressed': compressed,
            'original_size': len(mml_content),
            'compressed_size': len(compressed),
            'ratio': len(compressed) / len(mml_content),
            'strategy': self.strategy
        }
```

### Métriques de compression

#### Suivi des performances

**Ratios typiques** :
- **HTML vers MML** : 40-60% de réduction
- **MML vers MML-C** : 50-70% de réduction
- **Total** : 70-85% de réduction globale

**Temps de traitement** :
- **Petits documents** : <100ms
- **Documents moyens** : 200-500ms
- **Gros documents** : 1-5 secondes

## Étape 4 : Conversion MML-C → Morse (Transmission Layer)

### Mapping Morse optimisé

#### Sélection du mapping

**Critères** :
- **Fréquence d'usage** : Caractères MML-C courants prioritaires
- **Contexte** : Environnement de transmission (bruit, vitesse)
- **Évolution** : Apprentissage des patterns réels

```python
class MorseMapper:
    def __init__(self):
        self.mappings = {
            'standard': self.load_standard_mapping(),
            'optimized': self.load_optimized_mapping(),
            'emergency': self.load_emergency_mapping()
        }
        
    def select_mapping(self, transmission_context):
        """
        Sélection du mapping approprié
        """
        if transmission_context.get('emergency'):
            return self.mappings['emergency']
        elif transmission_context.get('optimize', True):
            return self.mappings['optimized']
        else:
            return self.mappings['standard']
```

### Génération des séquences Morse

#### Traitement caractère par caractère

```python
class MorseGenerator:
    async def generate_morse_sequence(self, mmlc_content, mapping, wpm=20):
        """
        Génération de la séquence Morse complète
        """
        morse_sequence = ""
        timing_info = []
        
        for char in mmlc_content:
            if char in mapping:
                morse_char = mapping[char]
                morse_sequence += morse_char + " "
                
                # Calcul du timing
                char_timing = self.calculate_char_timing(morse_char, wpm)
                timing_info.append(char_timing)
            else:
                # Caractère non mappé - fallback
                fallback_morse = self.fallback_to_standard_morse(char)
                morse_sequence += fallback_morse + " "
        
        return {
            'morse': morse_sequence.strip(),
            'total_time': sum(timing_info),
            'wpm': wpm,
            'character_count': len(mmlc_content)
        }
```

### Optimisations temporelles

#### Ajustement de la vitesse

**Adaptation automatique** :
- **Opérateur expérimenté** : Vitesse maximale (jusqu'à 40 WPM)
- **Conditions difficiles** : Réduction automatique (12-15 WPM)
- **Contenu complexe** : Lenteur pour précision

#### Gestion des pauses

**Insertions automatiques** :
- **Inter-lettre** : Espaces standard
- **Inter-mot** : Pauses plus longues
- **Inter-fragment** : Synchronisation réseau

## Étape 5 : Fragmentation et préparation réseau (Network Layer)

### Découpage en fragments transmissibles

#### Stratégies de fragmentation

**Fragmentation temporelle** :
```python
class Fragmenter:
    def create_transmission_fragments(self, morse_sequence, transport_type, max_time=300):
        """
        Création de fragments transmissibles
        """
        fragments = []
        current_fragment = ""
        current_time = 0
        
        # Paramètres selon le transport
        if transport_type == 'cw':
            max_time = min(max_time, 180)  # 3 minutes max pour CW
        elif transport_type == 'js8call':
            max_time = min(max_time, 45)   # 45 secondes pour JS8Call
        
        for element in self.parse_morse_elements(morse_sequence):
            element_time = self.estimate_element_time(element, transport_type)
            
            if current_time + element_time > max_time:
                # Fermeture du fragment actuel
                if current_fragment:
                    fragments.append(self.create_fragment(current_fragment, transport_type))
                
                # Nouveau fragment
                current_fragment = str(element)
                current_time = element_time
            else:
                current_fragment += str(element)
                current_time += element_time
        
        # Fragment final
        if current_fragment:
            fragments.append(self.create_fragment(current_fragment, transport_type))
        
        return fragments
```

### Enrichissement des fragments

#### Métadonnées de transmission

**Informations ajoutées** :
- **Identifiant unique** : UUID du fragment
- **Séquence** : Position dans le document (1/N, 2/N, etc.)
- **Checksum** : Validation d'intégrité
- **Priorité** : Niveau d'urgence
- **TTL** : Time To Live réseau

```python
def enrich_fragment(self, fragment_content, sequence_info):
    """
    Enrichissement d'un fragment avec métadonnées
    """
    fragment_id = str(uuid.uuid4())
    
    enriched = {
        'id': fragment_id,
        'sequence': sequence_info,
        'content': fragment_content,
        'checksum': self.calculate_checksum(fragment_content),
        'priority': sequence_info.get('priority', 'normal'),
        'ttl': 3600,  # 1 heure par défaut
        'timestamp': datetime.utcnow().isoformat()
    }
    
    return enriched
```

## Monitoring et métriques du pipeline

### Collecte de métriques

#### Points de mesure

**Par étape** :
- **Temps de traitement** : Durée de chaque transformation
- **Taille des données** : Évolution de la taille
- **Taux de compression** : Réduction effective
- **Erreurs détectées** : Problèmes rencontrés

```python
class PipelineMonitor:
    def __init__(self):
        self.metrics = {}
        
    def record_stage_metrics(self, stage_name, input_data, output_data, duration):
        """
        Enregistrement des métriques d'une étape
        """
        self.metrics[stage_name] = {
            'input_size': len(input_data),
            'output_size': len(output_data),
            'compression_ratio': len(output_data) / len(input_data) if input_data else 0,
            'processing_time': duration,
            'timestamp': datetime.utcnow()
        }
    
    def get_pipeline_summary(self):
        """
        Résumé global du pipeline
        """
        total_input = self.metrics[list(self.metrics.keys())[0]]['input_size']
        total_output = self.metrics[list(self.metrics.keys())[-1]]['output_size']
        total_time = sum(m['processing_time'] for m in self.metrics.values())
        
        return {
            'total_compression_ratio': total_output / total_input,
            'total_processing_time': total_time,
            'stage_metrics': self.metrics
        }
```

### Optimisation continue

#### Apprentissage automatique

**Analyse des patterns** :
- **Documents similaires** : Réutilisation des stratégies réussies
- **Erreurs récurrentes** : Ajustement automatique des paramètres
- **Performances** : Optimisation des seuils et paramètres

## Gestion d'erreurs et récupération

### Points de contrôle

#### Validation à chaque étape

**Contrôles automatiques** :
- **Taille** : Respect des limites
- **Intégrité** : Checksums valides
- **Conformité** : Format correct
- **Cohérence** : Données logiques

### Stratégies de récupération

#### En cas d'erreur

**Approches** :
- **Retry** : Nouvelle tentative avec paramètres ajustés
- **Fallback** : Utilisation d'une stratégie plus conservative
- **Partial success** : Transmission partielle du document
- **User notification** : Information de l'opérateur

```python
class ErrorRecoveryManager:
    async def handle_pipeline_error(self, error, context):
        """
        Gestion centralisée des erreurs de pipeline
        """
        error_type = self.classify_error(error)
        
        if error_type == 'size_limit':
            # Compression plus agressive
            await self.retry_with_higher_compression(context)
        elif error_type == 'transmission_failure':
            # Fragmentation plus fine
            await self.retry_with_smaller_fragments(context)
        elif error_type == 'validation_error':
            # Correction automatique
            await self.attempt_auto_correction(context)
        else:
            # Escalade vers l'utilisateur
            await self.notify_user(error, context)
```

## Interface utilisateur du pipeline

### Visualisation du progrès

#### Indicateurs en temps réel

**Informations affichées** :
- **Étape actuelle** : Chargement, conversion, compression, transmission
- **Progression** : Pourcentage et temps restant estimé
- **Métriques** : Taux de compression, taille finale
- **Statut** : Succès, avertissement, erreur

#### Détails techniques (mode avancé)

**Informations détaillées** :
- **Temps par étape** : Analyse des goulots
- **Taille à chaque étape** : Évolution des données
- **Paramètres utilisés** : Choix algorithmiques
- **Erreurs détectées** : Problèmes résolus automatiquement

## Conclusion : L'orchestration de la transformation

Le pipeline direct incarne la **puissance transformatrice** de notre système : prendre un document moderne complexe et le condenser en impulsions Morse transmissibles universellement.

Chaque étape représente un compromis calculé entre fidélité au contenu original et optimisation pour la transmission, aboutissant à une réduction de 70-85% de la taille originale tout en préservant l'essence informationnelle.

Cette architecture en pipeline ne se contente pas de fonctionner ; elle **s'améliore continuellement**, apprenant des erreurs passées et optimisant les performances futures. Elle transforme la complexité de la transmission en une expérience utilisateur fluide et fiable.
